<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>readme</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="README_files/libs/clipboard/clipboard.min.js"></script>
<script src="README_files/libs/quarto-html/quarto.js"></script>
<script src="README_files/libs/quarto-html/popper.min.js"></script>
<script src="README_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="README_files/libs/quarto-html/anchor.min.js"></script>
<link href="README_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="README_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="README_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="README_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="README_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<p>:new: [2023-10-26] <em>Added DINOv2 backbones with registers, following <a href="https://arxiv.org/abs/2309.16588">Vision Transformers Need Registers</a>.</em></p>
<section id="dinov2-learning-robust-visual-features-without-supervision" class="level1">
<h1>DINOv2: Learning Robust Visual Features without Supervision</h1>
<p><strong><a href="https://ai.facebook.com/research/">Meta AI Research, FAIR</a></strong></p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski</p>
<p>[<a href="https://arxiv.org/abs/2304.07193"><code>Paper #1</code></a>] <a href="https://arxiv.org/abs/2309.16588"><code>Paper #2</code></a>] [<a href="https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/"><code>Blog</code></a>] [<a href="https://dinov2.metademolab.com"><code>Demo</code></a>] [<a href="#citing-dinov2"><code>BibTeX</code></a>]</p>
<p>PyTorch implementation and pretrained models for DINOv2. For details, see the papers: <strong><a href="https://arxiv.org/abs/2304.07193">DINOv2: Learning Robust Visual Features without Supervision</a></strong> and <strong><a href="https://arxiv.org/abs/2309.16588">Vision Transformers Need Registers</a></strong>.</p>
<p>DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.</p>
<p>https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356</p>
<div data-align="center">
<p>Visualization of the three first principal components of the patch features of all frames, mapped to RGB values.</p>
</div>
<section id="pretrained-models" class="level2">
<h2 class="anchored" data-anchor-id="pretrained-models">Pretrained models</h2>
<table style="margin: auto">
<thead>
<tr>
<th>
model
</th>
<th>
# of<br>params
</th>
<th>
with<br>registers
</th>
<th>
ImageNet<br>k-NN
</th>
<th>
ImageNet<br>linear
</th>
<th>
download
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="right">
21 M
</td>
<td align="center">
:x:
</td>
<td align="right">
79.0%
</td>
<td align="right">
81.1%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="right">
21 M
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
79.1%
</td>
<td align="right">
80.9%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="right">
86 M
</td>
<td align="center">
:x:
</td>
<td align="right">
82.1%
</td>
<td align="right">
84.5%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="right">
86 M
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
82.0%
</td>
<td align="right">
84.6%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="right">
300 M
</td>
<td align="center">
:x:
</td>
<td align="right">
83.5%
</td>
<td align="right">
86.3%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="right">
300 M
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
83.8%
</td>
<td align="right">
86.7%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td align="right">
1,100 M
</td>
<td align="center">
:x:
</td>
<td align="right">
83.5%
</td>
<td align="right">
86.5%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth">backbone only</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td align="right">
1,100 M
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
83.7%
</td>
<td align="right">
87.1%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth">backbone only</a>
</td>
</tr>
</tbody>
</table>
<section id="pretrained-backbones-via-pytorch-hub" class="level3">
<h3 class="anchored" data-anchor-id="pretrained-backbones-via-pytorch-hub">Pretrained backbones (via PyTorch Hub)</h3>
<p>Please follow the instructions <a href="https://pytorch.org/get-started/locally/">here</a> to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.</p>
<p>A corresponding <a href="MODEL_CARD.md">model card</a> is included in the repository.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># DINOv2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>dinov2_vits14 <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vits14'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dinov2_vitb14 <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitb14'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>dinov2_vitl14 <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitl14'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>dinov2_vitg14 <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitg14'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># DINOv2 with registers</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>dinov2_vits14_reg <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vits14_reg'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>dinov2_vitb14_reg <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitb14_reg'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>dinov2_vitl14_reg <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitl14_reg'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>dinov2_vitg14_reg <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitg14_reg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pretrained-heads---image-classification" class="level3">
<h3 class="anchored" data-anchor-id="pretrained-heads---image-classification">Pretrained heads - Image classification</h3>
<table style="margin: auto">
<thead>
<tr>
<th rowspan="2">
backbone
</th>
<th rowspan="2">
with<br>registers
</th>
<th>
download
</th>
</tr>
<tr>
<th>
ImageNet
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="center">
:x:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth">4 layers</a>)
</td>
</tr>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear4_head.pth">4 layers</a>)
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="center">
:x:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth">4 layers</a>)
</td></tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear4_head.pth">4 layers</a>)
</td></tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="center">
:x:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth">4 layers</a>)
</td></tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear4_head.pth">4 layers</a>)
</td></tr>
<tr>
<td>
ViT-g/14
</td>
<td align="center">
:x:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth">4 layers</a>)
</td></tr>
<tr>
<td>
ViT-g/14
</td>
<td align="center">
:white_check_mark:
</td>
<td>
linear head (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_lreg4_inear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear4_head.pth">4 layers</a>)
</td></tr>
</tbody>
</table>
<p>The (full) classifier models can be loaded via PyTorch Hub:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># DINOv2</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>dinov2_vits14_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vits14_lc'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>dinov2_vitb14_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitb14_lc'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>dinov2_vitl14_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitl14_lc'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>dinov2_vitg14_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitg14_lc'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># DINOv2 with registers</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>dinov2_vits14_reg_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vits14_reg_lc'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>dinov2_vitb14_reg_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitb14_reg_lc'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>dinov2_vitl14_reg_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitl14_reg_lc'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>dinov2_vitg14_reg_lc <span class="op">=</span> torch.hub.load(<span class="st">'facebookresearch/dinov2'</span>, <span class="st">'dinov2_vitg14_reg_lc'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pretrained-heads---depth-estimation" class="level3">
<h3 class="anchored" data-anchor-id="pretrained-heads---depth-estimation">Pretrained heads - Depth estimation</h3>
<table style="margin: auto">
<thead>
<tr>
<th rowspan="2">
backbone
</th>
<th colspan="2">
download head
</th>
</tr>
<tr>
<th>
NYUd
</th>
<th>
KITTI
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
ViT-S/14 distilled
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth">DPT</a>
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth">DPT</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth">DPT</a>
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth">DPT</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth">DPT</a>
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth">DPT</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth">DPT</a>
</td>
<td>
linear (<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth">1 layer</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth">4 layers</a>), <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth">DPT</a>
</td>
</tr>
</tbody>
</table>
</section>
<section id="pretrained-heads---semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="pretrained-heads---semantic-segmentation">Pretrained heads - Semantic segmentation</h3>
<table style="margin: auto">
<thead>
<tr>
<th rowspan="2">
backbone
</th>
<th>
download model
</th>
<th colspan="2">
download head
</th>
</tr>
<tr>
<th>
ADE20K
</th>
<th>
ADE20K
</th>
<th>
VOC2012
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
ViT-S/14 distilled
</td>
<td>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth">multi-scale</a>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth">multi-scale</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth">multi-scale</a>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth">multi-scale</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth">multi-scale</a>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth">multi-scale</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth">Mask2Former</a>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth">multi-scale</a>
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth">linear</a>, <a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth">multi-scale</a>
</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="installation" class="level2">
<h2 class="anchored" data-anchor-id="installation">Installation</h2>
<p>The training and evaluation code requires PyTorch 2.0 and <a href="https://github.com/facebookresearch/xformers">xFormers</a> 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:</p>
<p><em><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html">conda</a></em> <strong>(Recommended)</strong> - Clone the repository and then create and activate a <code>dinov2</code> conda environment using the provided environment definition:</p>
<pre class="shell"><code>conda env create -f conda.yaml
conda activate dinov2</code></pre>
<p><em><a href="https://pip.pypa.io/en/stable/getting-started/">pip</a></em> - Clone the repository and then use the provided <code>requirements.txt</code> to install the dependencies:</p>
<pre class="shell"><code>pip install -r requirements.txt</code></pre>
<p>For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of <code>mmcv</code> and <code>mmsegmentation</code>) which are captured in the <code>extras</code> dependency specifications:</p>
<p><em><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html">conda</a></em> <strong>(Recommended)</strong>:</p>
<pre class="shell"><code>conda env create -f conda-extras.yaml
conda activate dinov2-extras</code></pre>
<p><em><a href="https://pip.pypa.io/en/stable/getting-started/">pip</a></em>:</p>
<pre class="shell"><code>pip install -r requirements.txt -r requirements-extras.txt</code></pre>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data preparation</h2>
<section id="imagenet-1k" class="level3">
<h3 class="anchored" data-anchor-id="imagenet-1k">ImageNet-1k</h3>
<p>The root directory of the dataset should hold the following contents:</p>
<ul>
<li><code>&lt;ROOT&gt;/test/ILSVRC2012_test_00000001.JPEG</code></li>
<li><code>&lt;ROOT&gt;/test/[..]</code></li>
<li><code>&lt;ROOT&gt;/test/ILSVRC2012_test_00100000.JPEG</code></li>
<li><code>&lt;ROOT&gt;/train/n01440764/n01440764_10026.JPEG</code></li>
<li><code>&lt;ROOT&gt;/train/[...]</code></li>
<li><code>&lt;ROOT&gt;/train/n15075141/n15075141_9993.JPEG</code></li>
<li><code>&lt;ROOT&gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG</code></li>
<li><code>&lt;ROOT&gt;/val/[...]</code></li>
<li><code>&lt;ROOT&gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG</code></li>
<li><code>&lt;ROOT&gt;/labels.txt</code></li>
</ul>
<p>The provided dataset implementation expects a few additional metadata files to be present under the extra directory:</p>
<ul>
<li><code>&lt;EXTRA&gt;/class-ids-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-ids-VAL.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-names-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-names-VAL.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-TEST.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-VAL.npy</code></li>
</ul>
<p>These metadata files can be generated (once) with the following lines of Python code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dinov2.data.datasets <span class="im">import</span> ImageNet</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> split <span class="kw">in</span> ImageNet.Split:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> ImageNet(split<span class="op">=</span>split, root<span class="op">=</span><span class="st">"&lt;ROOT&gt;"</span>, extra<span class="op">=</span><span class="st">"&lt;EXTRA&gt;"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    dataset.dump_extra()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that the root and extra directories do not have to be distinct directories.</p>
</section>
<section id="imagenet-22k" class="level3">
<h3 class="anchored" data-anchor-id="imagenet-22k">ImageNet-22k</h3>
<p>Please adapt the <a href="dinov2/data/datasets/image_net_22k.py">dataset class</a> to match your local setup.</p>
<p><br></p>
<p>:warning: To execute the commands provided in the next sections for training and evaluation, the <code>dinov2</code> package should be included in the Python module search path, i.e.&nbsp;simply prefix the command to run with <code>PYTHONPATH=.</code>.</p>
</section>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<section id="fast-setup-training-dinov2-vit-l16-on-imagenet-1k" class="level3">
<h3 class="anchored" data-anchor-id="fast-setup-training-dinov2-vit-l16-on-imagenet-1k">Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k</h3>
<p>Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:</p>
<pre class="shell"><code>python dinov2/run/train/train.py \
    --nodes 4 \
    --config-file dinov2/configs/train/vitl16_short.yaml \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
    train.dataset_path=ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
<p>Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.</p>
<p>The training code saves the weights of the teacher in the <code>eval</code> folder every 12500 iterations for evaluation.</p>
</section>
<section id="long-setup-training-dinov2-vit-l14-on-imagenet-22k" class="level3">
<h3 class="anchored" data-anchor-id="long-setup-training-dinov2-vit-l14-on-imagenet-22k">Long setup: training DINOv2 ViT-L/14 on ImageNet-22k</h3>
<p>Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:</p>
<pre class="shell"><code>python dinov2/run/train/train.py \
    --nodes 12 \
    --config-file dinov2/configs/train/vitl14.yaml \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
    train.dataset_path=ImageNet22k:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
<p>Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.</p>
<p>The training code saves the weights of the teacher in the <code>eval</code> folder every 12500 iterations for evaluation.</p>
</section>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:</p>
<section id="k-nn-classification-on-imagenet-1k" class="level3">
<h3 class="anchored" data-anchor-id="k-nn-classification-on-imagenet-1k">k-NN classification on ImageNet-1k</h3>
<pre class="shell"><code>python dinov2/run/eval/knn.py \
    --config-file &lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
    --pretrained-weights &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/knn \
    --train-dataset ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
    --val-dataset ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
</section>
<section id="logistic-regression-classification-on-imagenet-1k" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-classification-on-imagenet-1k">Logistic regression classification on ImageNet-1k</h3>
<pre class="shell"><code>python dinov2/run/eval/log_regression.py \
    --config-file &lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
    --pretrained-weights &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/logreg \
    --train-dataset ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
    --val-dataset ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
</section>
<section id="linear-classification-with-data-augmentation-on-imagenet-1k" class="level3">
<h3 class="anchored" data-anchor-id="linear-classification-with-data-augmentation-on-imagenet-1k">Linear classification with data augmentation on ImageNet-1k</h3>
<pre class="shell"><code>python dinov2/run/eval/linear.py \
    --config-file &lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
    --pretrained-weights &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt;/eval/training_24999/linear \
    --train-dataset ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
    --val-dataset ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
<p>We release the weights from evaluating the different models:</p>
<table style="margin: auto">
<tbody><tr>
<th>
model
</th>
<th>
with<br>registers
</th>
<th>
ImageNet<br>top-1
</th>
<th>
linear evaluation
</th>
</tr>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="center">
:x:
</td>
<td align="right">
81.1%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-S/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
80.8%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="center">
:x:
</td>
<td align="right">
84.5%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-B/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
84.4%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="center">
:x:
</td>
<td align="right">
86.3%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-L/14 distilled
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
86.5%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td align="center">
:x:
</td>
<td align="right">
86.5%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth">linear head weights</a>
</td>
</tr>
<tr>
<td>
ViT-g/14
</td>
<td align="center">
:white_check_mark:
</td>
<td align="right">
87.0%
</td>
<td>
<a href="https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth">linear head weights</a>
</td>
</tr>
</tbody></table>
<p>The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:</p>
<pre class="shell"><code>python dinov2/run/eval/linear.py \
    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \
    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \
    --train-dataset ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
    --val-dataset ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;</code></pre>
</section>
</section>
<section id="notebooks" class="level2">
<h2 class="anchored" data-anchor-id="notebooks">Notebooks</h2>
<p>A few notebooks are provided to help the community leverage the models and code:</p>
<ul>
<li>
<a href="https://github.com/facebookresearch/dinov2/blob/main/notebooks/depth_estimation.ipynb">Depth estimation</a> - How to load and use the depth heads in combination with a matching backbone via mmcv
</li>
<li>
<a href="https://github.com/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb">Semantic segmentation</a> - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K
</li>
</ul>
</section>
<section id="license" class="level2">
<h2 class="anchored" data-anchor-id="license">License</h2>
<p>DINOv2 code and model weights are released under the Apache License 2.0. See <a href="LICENSE">LICENSE</a> for additional details.</p>
</section>
<section id="contributing" class="level2">
<h2 class="anchored" data-anchor-id="contributing">Contributing</h2>
<p>See <a href="CONTRIBUTING.md">contributing</a> and the <a href="CODE_OF_CONDUCT.md">code of conduct</a>.</p>
</section>
<section id="citing-dinov2" class="level2">
<h2 class="anchored" data-anchor-id="citing-dinov2">Citing DINOv2</h2>
<p>If you find this repository useful, please consider giving a star :star: and citation :t-rex::</p>
<pre><code>@misc{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timothée and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  journal={arXiv:2304.07193},
  year={2023}
}</code></pre>
<pre><code>@misc{darcet2023vitneedreg,
  title={Vision Transformers Need Registers},
  author={Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  journal={arXiv:2309.16588},
  year={2023}
}</code></pre>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>