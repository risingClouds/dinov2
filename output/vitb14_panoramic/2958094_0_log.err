/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")
/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")
/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")
submitit ERROR (2024-10-31 16:36:13,911) - Submitted job triggered an exception
Traceback (most recent call last):
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/threading.py", line 324, in wait
    gotit = waiter.acquire(True, timeout)
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 224314) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/submitit/core/_submit.py", line 11, in <module>
    submitit_main()
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/submitit/core/submission.py", line 76, in submitit_main
    process_job(args.folder)
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/submitit/core/submission.py", line 69, in process_job
    raise error
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/submitit/core/submission.py", line 55, in process_job
    result = delayed.result()
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/submitit/core/utils.py", line 137, in result
    self._result = self.function(*self.args, **self.kwargs)
  File "/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/run/train/train.py", line 26, in __call__
    train_main(self.args)
  File "/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/train/train.py", line 313, in main
    do_train(cfg, model, resume=not args.no_resume)
  File "/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/train/train.py", line 222, in do_train
    for data in metric_logger.log_every(
  File "/home_data/home/v-luotao/projects/pretrain/dinov2/dinov2/logging/helpers.py", line 93, in log_every
    for obj in iterable:
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1285, in _get_data
    success, data = self._try_get_data()
  File "/public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 224314) exited unexpectedly
slurmstepd: error: Detected 10 oom-kill event(s) in step 2958094.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: bme_gpu01: task 0: Out Of Memory
srun: Terminating job step 2958094.0
