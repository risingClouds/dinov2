submitit INFO (2024-10-31 16:12:09,524) - Starting with JobEnvironment(job_id=2958086, hostname=bme_gpu01, local_rank=0(1), node=0(1), global_rank=0(1))
submitit INFO (2024-10-31 16:12:09,526) - Loading pickle: /home_data/home/v-luotao/projects/pretrain/dinov2/output/vitb14_panoramic/2958086_submitted.pkl
I20241031 16:13:55 211891 dinov2 config.py:59] git:
  sha: eff45652f3648a0aa79e322f527cd16129d945a5, status: has uncommitted changes, branch: main

I20241031 16:13:55 211891 dinov2 config.py:60] comment: 
config_file: dinov2/configs/train/vitb14_panoramic.yaml
eval: 
eval_only: False
exclude: 
ngpus: 1
no_resume: False
nodes: 1
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=dinov2/data/ImageNet-1k-style:extra=dinov2/data/extra', 'train.output_dir=/home_data/home/v-luotao/projects/pretrain/dinov2/output/vitb14_panoramic']
output_dir: /home_data/home/v-luotao/projects/pretrain/dinov2/output/vitb14_panoramic
partition: bme_gpu
timeout: 2800
use_volta32: False
I20241031 16:13:55 211891 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 1.530931089239486e-05
I20241031 16:13:55 211891 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 6
  dataset_path: ImageNet:split=TRAIN:root=dinov2/data/ImageNet-1k-style:extra=dinov2/data/extra
  output_dir: /home_data/home/v-luotao/projects/pretrain/dinov2/output/vitb14_panoramic
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: swiglufused
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 500
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 1.530931089239486e-05
  warmup_epochs: 80
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 896
  local_crops_size: 392
evaluation:
  eval_period_iterations: 12500

I20241031 16:13:55 211891 dinov2 vision_transformer.py:125] using SwiGLU layer as FFN
I20241031 16:13:57 211891 dinov2 vision_transformer.py:125] using SwiGLU layer as FFN
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20241031 16:13:59 211891 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:96] OPTIONS -- IBOT -- loss_weight: 1.0
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20241031 16:14:00 211891 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20241031 16:14:01 211891 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20241031 16:14:01 211891 py.warnings warnings.py:109] /public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20241031 16:14:01 211891 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=768, out_features=4096, bias=True)
                  (w3): Linear(in_features=2048, out_features=768, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20241031 16:14:01 211891 dinov2 param_groups.py:54] chunked fsdp
I20241031 16:14:01 211891 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241031 16:14:01 211891 dinov2 param_groups.py:64] else code branch
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241031 16:14:01 211891 dinov2 param_groups.py:64] else code branch
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20241031 16:14:01 211891 dinov2 ssl_meta_arch.py:378] fusing param groups
I20241031 16:14:01 211891 dinov2 train.py:102] Schedulers ready.
I20241031 16:14:01 211891 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20241031 16:14:01 211891 dinov2 augmentations.py:34] ###################################
I20241031 16:14:01 211891 dinov2 augmentations.py:35] Using data augmentation parameters:
I20241031 16:14:01 211891 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20241031 16:14:01 211891 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20241031 16:14:01 211891 dinov2 augmentations.py:38] local_crops_number: 8
I20241031 16:14:01 211891 dinov2 augmentations.py:39] global_crops_size: 896
I20241031 16:14:01 211891 dinov2 augmentations.py:40] local_crops_size: 392
I20241031 16:14:01 211891 dinov2 augmentations.py:41] ###################################
I20241031 16:14:01 211891 dinov2 loaders.py:84] using dataset: "ImageNet:split=TRAIN:root=dinov2/data/ImageNet-1k-style:extra=dinov2/data/extra"
I20241031 16:14:01 211891 dinov2 loaders.py:89] # of dataset samples: 7,720
I20241031 16:14:01 211891 dinov2 loaders.py:122] sampler: sharded infinite
I20241031 16:14:01 211891 dinov2 loaders.py:206] using PyTorch data loader
W20241031 16:14:01 211891 py.warnings warnings.py:109] /public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

I20241031 16:14:01 211891 dinov2 loaders.py:221] infinite data loader
I20241031 16:14:01 211891 dinov2 train.py:217] Starting training from iteration 0
W20241031 16:15:10 211891 py.warnings warnings.py:109] /public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage_data_ptr = tensors[0].storage().data_ptr()

W20241031 16:15:10 211891 py.warnings warnings.py:109] /public_bme/data/v-luotao/conda_envs/lt-dinov2/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  if x.storage().data_ptr() != storage_data_ptr:

submitit ERROR (2024-10-31 16:17:05,029) - Submitted job triggered an exception
E20241031 16:17:05 211891 submitit submission.py:68] Submitted job triggered an exception
